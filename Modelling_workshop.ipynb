{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb1331df",
   "metadata": {},
   "source": [
    "# Modelling Workshop\n",
    "\n",
    "By Juan Vidal-Perez (https://colab.research.google.com/drive/1uUo-dx6MNr8Ave6niGmsOFpGOPiuMST2#scrollTo=eb1331df)\n",
    "\n",
    "## Getting started\n",
    "To execute a \"cell\", you'll press Shift-Enter.\n",
    "\n",
    "### Hosted Colab\n",
    "If you're working from a hosted colab (recommended):\n",
    "1. File > Save a copy in Drive\n",
    "2. Connect (top right) > Connect to a hosted runtime (GPU)\n",
    "\n",
    "### Run locally as a Jupyter Notebook\n",
    "\n",
    "You can also open the notebook in jupyter notebook or a locally hosted colab. To run locally, you can open a terminal window and download the code with:\n",
    "\n",
    "\n",
    "```\n",
    "pip install notebook\n",
    "git clone https://github.com/Juan-VidalPerez/Modelling_workshop\n",
    "cd Modelling_workshop\n",
    "jupyter notebook Modelling_workshop.ipynb\n",
    "\n",
    "```\n",
    "\n",
    "## Hints\n",
    "\n",
    "**Need help?** To view information about a function or class, type `?` before the name of the function or class (For example, `?print` or `?str` will cause information about the print function or the string datatype to pop up on the right.)\n",
    "\n",
    "**Table of Contents:** To view the Table of contents for the tutorial, you can click the icon in the top left with three dots next to three lines.\n",
    "\n",
    "## Resources\n",
    "\n",
    "These worskshop is loosely based on \"Ten simple rules for the computational modeling of behavioral data\" by Wilson and Collins (https://elifesciences.org/articles/49547). The paper explores the topic of modelling in more depth, so you can take a look if you are interested! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebe5791",
   "metadata": {},
   "source": [
    "# INTRODUCTION AND OVERVIEW\n",
    "\n",
    "The goal of computational modeling in behavioral science is to use precise mathematical models to make better sense of behavioral data. The behavioral data most often come in the form of choices, but can also be reaction times, eye movements, or other easily observable behaviors, and even neural data. The models come in the form of mathematical equations that link the experimentally observable variables (e.g. stimuli, outcomes, past experiences) to behavior in the immediate future. In this sense, computational models instantiate different ‘algorithmic hypotheses’ about how behavior is generated. \n",
    "\n",
    "Computational modelling is a very extensive topic, but in this workshop we will go over three uses of modelling that dominate the literature:\n",
    "\n",
    "* **Part 1: Simulations.** We will learn how to simulate data from a *synthetic agent* playing an n-armed bandit task. We will understand how the Rescorla-Wagner model works, and how it can inform action selection. Finally, we will get an intuition of how the parameters of the model can have an impact on behavior.\n",
    "\n",
    "* **Part 2: Parameter fitting.** We will fit a dataset to our model and take a look at what parameters best explain the behavior in the dataset. We will also take a look at best practices, such as parameter recovery.\n",
    "\n",
    "* **Part 3: Model comparison.** We will learn how to decide which of several models best explains a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701053b5",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "Go ahead and run the cell below to load the required packages (make sure that you have downloaded the entire folder with all the code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c246f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy\n",
    "import plotting\n",
    "import models\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9bb95e",
   "metadata": {},
   "source": [
    "## PART 0: WELCOME TO REINFORCEMENT LEARNING (RL)\n",
    "\n",
    "In this workshop, we will focus on Reinforcement Learning (RL) tasks. RL is a type of learning where individuals learn to make decisions by receiving rewards or punishments for their actions. By modeling RL, we can study how humans and animals learn from experience, optimize their actions to achieve goals, and adapt to changing environments. This approach provides insights into motivation, learning processes, and decision-making strategies.\n",
    "\n",
    "### The n-armed bandit problem\n",
    "\n",
    "The n-armed bandit task is a classic problem in reinforcement learning and behavioral psychology. Imagine you are in a casino with several slot machines (also called \"bandits\"), each with a different probability of paying out a reward when played. Your goal is to maximize your total reward over a series of plays. However, you don't know the payout probabilities of the machines in advance, so you must learn which machines are the most rewarding through trial and error.\n",
    "\n",
    "You can design bandit tasks with different number of bandits and reward policies (i.e., constant reward probabilities, drifting, reversal tasks...). Here we will focuss on a two-armed badnit task (i.e., with only two bandits) with constant reward probabilities.\n",
    "\n",
    "### Rescorla-Wagner\n",
    "\n",
    "The Rescorla-Wagner rule is a model of classical conditioning that describes how the strength of an association between a stimulus and an outcome is updated over time. In the context of a two-armed bandit task, this rule can be used to model how individuals learn to associate each of the two options (or \"arms\") with their respective rewards.\n",
    "\n",
    "The Rescorla-Wagner Model includes three main variables:\n",
    "\n",
    "* $Q_t(a)$: the Q-value of bandit *a* on trial *t*, which represents the expected reward associated with that bandit in that trial (i.e., the prediction of how likely the bandit is to give a reward). \n",
    "\n",
    "* $R_t$: the outcome (i.e., reward or nothing) received after choosing a bandit on trial *t*.\n",
    "\n",
    "* $\\alpha$: the learning rate, which determines how quickly the model updates the Q-values based on new information.\n",
    "\n",
    "The Rescorla-Wagner rule relies on a prediction error ($\\delta_t$), which is the difference between the actual reward ($R_t$) and the predicted reward ($Q_t$) of the chosen bandit ($a_t$) on trial *t*:\n",
    "\n",
    "$$\\delta_t = R_t - Q_t(a_t)$$\n",
    "\n",
    "With the reward prediction error $\\delta_t$, we can update the value of the chosen option $Q_t(a_t)$ to obtain $Q_{t+1}(a_t)$: \n",
    "\n",
    "$$Q_{t+1}(a_t) = Q_t(a_t) + \\alpha*\\delta_t$$\n",
    "\n",
    "For the update, we *could* simply add the reward prediction error $\\delta_t$ to our old value estimate $Q_t$, to immediately arrive at $R_t$, the true outcome. However, we don't really want to move *all the way* from the old value $V_t$ to the actual outcome $r_t$. We only want to move *a little bit*, in order to keep around some of the things we have already learned in the past.\n",
    "\n",
    "This is the reason why we first multiply $\\delta_t$ by $\\alpha$, the *learning rate*, before we add it to $Q_t(a_t)$. (The learning rate $\\alpha$ is a number between 0 and 1 that \"scales\" the reward prediction error. For example, if $\\alpha=0.25$, we move a quarter of the way from $Q_t$ to $R_t$.)\n",
    "\n",
    "\n",
    "\n",
    "We'll see what this looks like for a single trial. First, we'll define an agent in the cell below. Then, we'll see how Q-values are modified after a single trial containing one reward outcome.\n",
    "\n",
    "**Exercise 1:** Complete the code below to implement the Rescorla-Wagner update for a single trial.\n",
    "\n",
    "**Exercise 2:** change the learning rate to different values. How does the update change? What happens when the elarning rate is 1? And when it is 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a05dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Agent behavior: **One trial.**\n",
    "\n",
    "# We define the variables\n",
    "Q0=0.5 # Original Q-value\n",
    "R=1 # Bandit outcome (non-reward=0, reward=1)\n",
    "alpha=0.25 # Learning rate between 0 and 1\n",
    "\n",
    "# Rescorla-Waner update\n",
    "Q = ## COMPLETE THE RESCORLA WAGNER RULE ##\n",
    "print(\"Agent's initial Q-value:\", Q0)\n",
    "print(\"Agent's new Q-values:\", Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b122cf",
   "metadata": {},
   "source": [
    "### Action choice\n",
    "\n",
    "But how do people use he bandits' Q-values to guide their decisions? A choice rule would be to simply select the bandit with the highest Q-value, since this the bandit that is expected to give more rewards (based on the information gathered so far). However, most people do not do this. Our Q-values may be wrong, so it is useful to occasionally make 'mistakes' (or explore) by choosing low-value options (i.e., to explore). One choice rule with these properties is known as the ‘softmax’ choice rule, which chooses bandit *a^i* with probability:\n",
    "\n",
    "$$p(a^i)=\\frac{exp(\\beta*Q_t(a^i))}{\\sum_{k=1}^K exp(\\beta*Q_t(a^k))}$$\n",
    "\n",
    "In this mathematical notation, the numerator contains the exponantial of the Q-value of bandit *a^i* (multiplied by a parameter $\\beta$, while the denominator contains the sum of the same exponential (and the same $\\beta$ parameter) applied to all available bandits. In our two bandit task, this is equivalent to writting:\n",
    "\n",
    "$$p(a^1)=\\frac{1}{1 + exp(\\beta*(Q_t(a^2)-Q_t(a^1))}$$\n",
    "\n",
    "$$p(a^2)=1-p(a^1)$$\n",
    "\n",
    "As you can see, the probability of choosing a given bandit (i.e., bandit $a^1$) depends on the difference in Q-values bwteeen the bandits ($Q_t(a^2)-Q_t(a^1)$).\n",
    "\n",
    "$\\beta$ is what we call the ‘inverse temperature’ parameter. Let's try to figure out what it does. Let's plot the different probabilities of choosing one bandit as a function of the difference in Q values between the two bandits.\n",
    "\n",
    "**Exercise 1:** Complete the code below to implement the softmax update the different Q-value differences (**Q_diff**).\n",
    "\n",
    "**Exercise 2:** change the inverse temperature parameter ($\\beta$) and see how the function changes. What happens if you use a $\\beta$ of 0? And a very large $\\beta$ (e.g., of 1000)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d94864",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta=10 # Choose a value of the inverse temperature parameters\n",
    "Q_diff=np.arange(-1,1,0.001) # Compute the different Q-value differences \n",
    "p_choice= ## COMPLETE THE SOFTMAX CHOICE RULE ##\n",
    "\n",
    "#Plot \n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(-Q_diff,p_choice, color='k',linewidth=3)\n",
    "plt.grid()\n",
    "plt.xlabel(f'Q($a^1$)-Q($a^2$)')\n",
    "plt.ylabel(f'Probability of choosing $a^1$')\n",
    "plt.title('Softmax choice rule')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28763890",
   "metadata": {},
   "source": [
    "It controls the level of stochasticity in the choice, ranging from β=0 for completely random responding and β=∞ for deterministically choosing the highest value option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a60ec1",
   "metadata": {},
   "source": [
    "## PART 1: SIMULATIONS\n",
    "\n",
    "A really important step in computational modelling is to create fake, or surrogate data. That is, you should use the models that you are considering to simulate the behavior of participants in the experiment, and to observe how behavior changes with different models, different model parameters, and different variants of the experiment. This is important since it will allow us to form concrete predictions about what our models predict, which we can then compare with actual behavioral data. \n",
    "\n",
    "Below you can see a function simulating our Rescorla-Wagner model (RW model). The function taskes as input the learning rate (**alpha**) and the inverse temperature (**beta**) parameters. It outputs a numpy array containing a the choice made on each trial based on the model (as well, as their associated outcomes, the accuracy of each choice and the Q-values of each bandit).\n",
    "\n",
    "Take a look at the function to see if you understand it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77623ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RW_simulation (alpha: float, beta: float,\n",
    "                       p_reward: list[float]=[0.25, 0.75], n_blocks: int=20, n_trials: int=30):\n",
    "    \"\"\"\n",
    "    Simulates an n-armed bandit task.\n",
    "\n",
    "    Input:\n",
    "    alpha: learning rate for the simulation.\n",
    "    beta: inverse temperature parameter for softmax.\n",
    "    \n",
    "    Optional inputs:\n",
    "    p_reward: list specifying reward probabilities of the bandits (default: [0.25, 0.75])\n",
    "    n_blocks: number of blocks in the task (default: 10)\n",
    "    n_trials: number of trials per block in the task (default: 30)\n",
    "      \n",
    "\n",
    "    Returns:\n",
    "    output: numpy array with the results, with the following structure\n",
    "        - Dimension 0: block number\n",
    "        - Dimension 1: trial number\n",
    "        - Dimension 2: [choices,outcomes,accuracy,q_values]\n",
    "    \"\"\"\n",
    "    n_bandits = len(p_reward) #number of bandits\n",
    "\n",
    "    output = np.full([n_blocks,n_trials, 3+n_bandits], np.nan) #initialize numpy array to store outputs\n",
    "    \n",
    "    for bb in range(n_blocks):\n",
    "        Q_values=0.5*np.ones(n_bandits) # Initialize Q-values at the beginning of each block\n",
    "        for tt in range(n_trials):\n",
    "            # Calculate the probability of choosing each bandit (based on softmax)\n",
    "            p_choice = softmax(beta*Q_values)\n",
    "            # Choose one of the bandits given the calculated probabilities\n",
    "            choice = np.random.choice(n_bandits, p=p_choice)\n",
    "            \n",
    "            # Generate an outcome from the bandit\n",
    "            outcome = np.random.choice(2, p=[1-p_reward[choice], p_reward[choice]])\n",
    "            \n",
    "            # Update the Q-value of the chosen bandit based on the outcome\n",
    "            Q_values[choice]= ## COMPLETE THE LEARNING RULE BASED ON RESCORLA WAGNER ##\n",
    "            \n",
    "            # Check accuracy (i.e., whether the chosen bandit is the most rewarding one)\n",
    "            accuracy = choice==p_reward.index(max(p_reward))\n",
    "            \n",
    "            # Store information in numpy array\n",
    "            output[bb,tt,0]=choice\n",
    "            output[bb,tt,1]=outcome\n",
    "            output[bb,tt,2]=accuracy\n",
    "            output[bb,tt,3:]=Q_values\n",
    "            \n",
    "    return output\n",
    " \n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    # Subtract the max value from each element for numerical stability\n",
    "    x_max = np.max(x)\n",
    "    e_x = np.exp(x - x_max)\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c0786a",
   "metadata": {},
   "source": [
    "Now let's simulate an agent playing a block of 30 trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed7a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=RW_simulation (alpha=0.3,beta=2, n_trials=30, n_blocks=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a065710",
   "metadata": {},
   "source": [
    "### Latent variables\n",
    "\n",
    "To check how the agent performs, we can take a look at how its Q-values for the different bandits evolved throughout the block. Execute the cell below to plot such values.\n",
    "\n",
    "**Questions:**\n",
    "* What do the orange and blue line represent?\n",
    "* What do the vertical green and red ticks represent?\n",
    "* What is the relationship between the orange/blue and red/green lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6604e61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_q_values(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbe3731",
   "metadata": {},
   "source": [
    "As you can see, we can use our model to compute  to compute the values of hidden variables (for example values of different choices) that are not immediately observable in the behavioral data, but which the theory assumes are important for the computations occurring in the brain. Latent variable inference is especially useful in neuroimaging where it is used to help search for the neural correlates of the model.\n",
    "\n",
    "### Behavioral signatures\n",
    "\n",
    "In modelling it is also very important to understand what behaviors are predicted by the model, since this is the information we will be able to directly obtain from participants. Finding qualitative signatures (and there will often be more than one) of the model is crucial. By studying these measures with simulated data, you will have greater intuition about what is going on when you use the same model-independent measures to analyze real behavior.\n",
    "\n",
    "For example, we can take a look at how the average choice accuracy evolves as the agent plays with the bandits. We define choice accuracy as the probability of choosing the most rewarding bandit (i.e., the bandit with the highest reward probability). Let's simulate a bandit playing many blocks of the task, and then let's see how its average accuracy evolves as it progresses through the block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd7961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=RW_simulation (alpha=0.3,beta=2, n_trials=30, n_blocks=500)\n",
    "plotting.plot_accuracy(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f89dce",
   "metadata": {},
   "source": [
    "**Question**: is the agent \"learning\"? How can you tell?\n",
    "\n",
    "**Exercise:** check what happens as you change the learning rate of the simulation. What happens when you change the inverse temperature?\n",
    "\n",
    "Another signature that is commonly used in RL is the probability of repeating an action (p(repeat)) depending on whether the last outcome was a reward or not (should I change my behavior in response to feedback?). Let's plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec4900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=RW_simulation (alpha=0.3,beta=2, n_trials=30, n_blocks=30)\n",
    "plotting.plot_prepeat(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b79aa2",
   "metadata": {},
   "source": [
    "**Question**: is the agent \"learning\"? How can you tell?\n",
    "\n",
    "**Exercise:** check again what happens as you change the parameters of the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1df7261",
   "metadata": {},
   "source": [
    "## PART 2: PARAMETER FITTING\n",
    "\n",
    "A key component of computational modeling is estimating the values of the parameters that best describe your behavioral data. There are a number of different ways of estimating parameters, but here we focus on the maximum-likelihood approach.\n",
    "\n",
    "In the maximum likelihood approach to model fitting, our goal is to find the parameter values of model m ($\\theta_m$) that maximize the likelihood of the data, $d_{1:T}$, given the parameters, $p(d_{1:T}|\\theta_m)$. For instance, in our RW model the parameters are the learning rate and inverse tempretature ($\\theta_{RW}={\\alpha,\\beta}$, but other models may have different parameters.\n",
    "\n",
    "Maximizing the likelihood is equivalent to maximizing the log of the likelihood, $LL=log(p(d_{1:T}|\\theta_m))$, which is numerically more tractable. (The likelihood is a product of many numbers smaller than 1, which can be rounded to 0 with limited precision computing. By contrast, the log-likelihood is a sum of negative numbers, which is usually tractable and will not be rounded to 0.) A simple mathematical derivation shows that this log-likelihood can be written in terms of the choice probabilities of the individual model as:\n",
    "\n",
    "$$LL=log(p(d_{1:T}|\\theta_m,m))=\\sum_{t=1}^{T}log(p(c_t|p(d_{1:t-1},\\theta_m,m))$$\n",
    "\n",
    "where $p(c_t|p(d_{1:t-1},\\theta_m,m)$ is the probability of each individual choice ($c_t$) given the parameters of the model ($\\theta_m$) and the information available up to that choice ($d_{1:t-1}$). The parameters that we can change (to explain the behavior of different participants) ($\\theta_m$) are usually called the \"free parameters\" of the model.\n",
    "\n",
    "Let's create a function that calculates the loglikelihood of the data given based on our RW, given a set of parameters. Take a look at the code below and complete the computation of the loglikelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9d0eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logl_RW (parameters: list[float],\n",
    "                       data):\n",
    "    \"\"\"\n",
    "    Calculates the loglikelihood of the dataset based on the Rescorla-Wagner model.\n",
    "\n",
    "    Inputs:\n",
    "    parameters: list with the following structure:\n",
    "        parameters[0]: learning rate\n",
    "        parameters[1]:inverse temperature\n",
    "    data: dataset structured as the output of bandit_simulations\n",
    "      \n",
    "\n",
    "    Returns:\n",
    "    ll: loglikelihood of choices given parameters\n",
    "    \"\"\"\n",
    "    # Unpack parameters\n",
    "    alpha=parameters[0]\n",
    "    beta=parameters[1]\n",
    "    \n",
    "    #Read number of bandits\n",
    "    n_bandits = data.shape[2]-3\n",
    "    \n",
    "    #Extract data\n",
    "    choice = data[:,:,0].astype(int)\n",
    "    outcome = data[:,:,1]\n",
    "    \n",
    "    #Initialize numpy array to store likelihoods\n",
    "    likelihood = np.full(data.shape[0:2], np.nan)\n",
    "    \n",
    "    for bb in range(data.shape[0]):\n",
    "        Q_values=0.5*np.ones(n_bandits) # Initialize Q-values at the beginning of each block\n",
    "        for tt in range(data.shape[1]):\n",
    "            # Calculate probability of chosen bandit (based on Q-values)\n",
    "            p_choice = softmax(beta*Q_values)\n",
    "            \n",
    "            # Update Q-values based on outcome\n",
    "            Q_values[choice[bb,tt]] += alpha*(outcome[bb,tt]-Q_values[choice[bb,tt]])\n",
    "            \n",
    "            # Store likelihood of choice    \n",
    "            likelihood[bb,tt] = ## COMPLETE , WHAT'S THE LIKELIHOOD OF THE SELECTED CHOICE? ##\n",
    "\n",
    "    ll= ## COMPLETE THE SUM OF LOGLIKELIHOODS OF ALL CHOICES ##\n",
    "\n",
    "    return ll\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8228b951",
   "metadata": {},
   "source": [
    "Let's simulate an agent with a given pair of parameters and calculate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0a4285",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=RW_simulation (alpha=0.3,beta=2, n_trials=30, n_blocks=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd3320e",
   "metadata": {},
   "source": [
    "Now, let's calculate the loglikelihood of the dataset given different sets of parameters.\n",
    "\n",
    "**Exercise:** what happens to the LL when you use parameters closer to the generating parameters? And when you use parameters that are very different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8045c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.3\n",
    "beta=2\n",
    "LL=logl_RW ([alpha,beta],dataset)\n",
    "print(\"Learning rate used to calculate LL:\", alpha)\n",
    "print(\"Inverse temperature used to calculate LL:\", beta)\n",
    "print(\"Loglikelihood of dataset given parameters:\", LL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bb2df9",
   "metadata": {},
   "source": [
    "So, we need to find the parameters that maximize the loglikelihood of the data given the model. In principle, finding the maximum likelihood parameters is as ‘simple’ as maximizing LL. In practice, of course, finding the maximum of a function is not a trivial process. The simplest approach would be to search the entire parameter space (i.e., try all the possible parameter combinations) and determine which set of parameters maximize the likelihood.\n",
    "\n",
    "Let's try to do that. Run the cell bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78989f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate learning rates between 0 and 1 (step=0.01)\n",
    "alpha=np.arange(0,1,0.01)\n",
    "# Generate inverse temperatures between 0 and 10 (step=0.1)\n",
    "beta=np.arange(0,10,0.1)\n",
    "# Initialize the numpy array to store LLs\n",
    "LL=np.full([len(alpha), len(beta)], np.nan)\n",
    "\n",
    "start_time = time.time()\n",
    "# Try every learning rate/inverse temperature combination\n",
    "for a in range(len(alpha)):\n",
    "    for b in range(len(beta)):\n",
    "        LL[a,b]=logl_RW([alpha[a],beta[b]],dataset) # Calculate loglikelihood based on each set of parameters\n",
    "        \n",
    "elapsed_time = time.time() - start_time # Time how long the code takes to run\n",
    "print(f\"\\rElapsed time: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "#Plot results\n",
    "plotting.plot_parhm (LL,alpha,beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625ccb9a",
   "metadata": {},
   "source": [
    "You can see that running the previous code took several seconds/minutes. And this is with a model with just two parameters, but many models have many more parameters, which means that finding the maximum likelihood (ML) parameetrs would be very costly.\n",
    "\n",
    "Fortunately, a number of tools exist for finding local maxima (and minima) of functions quickly using variations on gradient ascent (or descent). For example, scipy.optimize function can use a variety of sophisticated optimization algorithms to find the **minimum** of a function. So long as one remembers to feed scipy.optimize the negative log-likelihood (whose minimum is at the same parameter values as the maximum of the positive log-likelihood), using tools such as scipy.optimize can greatly speed up model fitting. \n",
    "\n",
    "Let's try to use this function to find the ML parameters:\n",
    "\n",
    "**IMPORTANT: remember this function finds minima (NOT maxima), so we need to go back to our logl_RW function and put a minus sign before the output!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab40faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logl_RW (parameters: list[float],\n",
    "                       data):\n",
    "    \"\"\"\n",
    "    Calculates the loglikelihood of the dataset based on the Rescorla-Wagner model.\n",
    "\n",
    "    Inputs:\n",
    "    parameters: list with the following structure:\n",
    "        parameters[0]: learning rate\n",
    "        parameters[1]:inverse temperature\n",
    "    data: dataset structured as the output of bandit_simulations\n",
    "      \n",
    "\n",
    "    Returns:\n",
    "    ll: negative loglikelihood of choices given parameters\n",
    "    \"\"\"\n",
    "    # Unpack parameters\n",
    "    alpha=parameters[0]\n",
    "    beta=parameters[1]\n",
    "    \n",
    "    #Read number of bandits\n",
    "    n_bandits = data.shape[2]-3\n",
    "    \n",
    "    #Extract data\n",
    "    choice = data[:,:,0].astype(int)\n",
    "    outcome = data[:,:,1]\n",
    "    \n",
    "    #Initialize numpy array to store likelihoods\n",
    "    likelihood = np.full(data.shape[0:2], np.nan)\n",
    "    \n",
    "    for bb in range(data.shape[0]):\n",
    "        Q_values=0.5*np.ones(n_bandits) # Initialize Q-values at the beginning of each block\n",
    "        for tt in range(data.shape[1]):\n",
    "            # Calculate probability of chosen bandit (based on Q-values)\n",
    "            p_choice = softmax(beta*Q_values)\n",
    "            \n",
    "            # Update Q-values based on outcome\n",
    "            Q_values[choice[bb,tt]] += alpha*(outcome[bb,tt]-Q_values[choice[bb,tt]])\n",
    "            \n",
    "            # Store likelihood of choice    \n",
    "            likelihood[bb,tt] = p_choice[choice[bb,tt]] # COMPLETE\n",
    "\n",
    "    ll = np.nansum(np.log(likelihood)) # store the sum of loglikelihoods\n",
    "            \n",
    "    return -ll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a253ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bounds of the parameters (the function will look for a minima within those bounds)\n",
    "bounds=scipy.optimize.Bounds(ub=np.array([1,10]),\n",
    "                                          lb=np.zeros(2))\n",
    "\n",
    "# Define a starting point to start the search (we can just uniform samples of points within the bounds)\n",
    "init_par=np.array([np.random.rand(1), 10*np.random.rand(1)])\n",
    "\n",
    "# Let's calculate the ML parameters\n",
    "mll=scipy.optimize.minimize(logl_RW, init_par.ravel(), args=dataset,\n",
    "                     method='L-BFGS-B', bounds=bounds)\n",
    "\n",
    "print(f'Maximum loglikelihood= {-mll.fun:.3f}')\n",
    "print('ML parameters')\n",
    "print(f'  -Learning rate= {mll.x[0]:.2f}')\n",
    "print(f'  -Inverse temperature parameter= {mll.x[1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07186cd",
   "metadata": {},
   "source": [
    "**Question:** did the function found the correct maximum likelihood parameters? What happens if you run the function multiple times, do you always get the same result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba31fe3e",
   "metadata": {},
   "source": [
    "A key limitation of optimization algorithms is that they are only guaranteed to find local minima, which are not guaranteed to be the global minima corresponding to the best fitting parameters. One way to mitigate this issue is to run the fitting procedure multiple times with random initial conditions, recording the best fitting log-likelihood for each run. The best fitting parameters are then the parameters corresponding to the run with the highest log-likelihood. There is no hard-and-fast rule for knowing how many starting points to use in a given situation, besides the fact that more complex models will require more starting points.\n",
    "\n",
    "Let's now define a function that tries to find the maximum likelihood parameters based on our RW model, using several attempts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f40816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_RWmodel(dataset, n_attempts: int =10, ub_beta: float =10):\n",
    "    \"\"\"\n",
    "    Finds the maximum-likelihood parameters based on the Rescorla-Wagner model.\n",
    "\n",
    "    Inputs:\n",
    "    dataset: dataset structured as the output of bandit_simulations\n",
    "    \n",
    "    Optional inputs:\n",
    "    n_attempts: number of attempts to find the minimum (to find the global minimum) (default=10)\n",
    "    ub_beta: upper bound of the beta parameter (default=10)\n",
    "      \n",
    "\n",
    "    Returns:\n",
    "    parameters: maxmum likelihood parameters.\n",
    "    mll: loglikelihood of choices given the maximum likelihood parameters\n",
    "    \"\"\"\n",
    "    tmp_param=[]\n",
    "    tmp_mll=[]\n",
    "    bounds=scipy.optimize.Bounds(ub=np.array([1,ub_beta]),\n",
    "                                          lb=np.zeros(2))\n",
    "    init_par=np.array([np.random.rand(n_attempts), ub_beta*np.random.rand(n_attempts)])\n",
    "    for aa in range(n_attempts):\n",
    "        \n",
    "        mle=scipy.optimize.minimize(logl_RW, init_par[:,aa], args=dataset,\n",
    "                     method='L-BFGS-B', bounds=bounds)\n",
    "        tmp_param.append(mle.x)\n",
    "        tmp_mll.append(mle.fun)\n",
    "\n",
    "    parameters=tmp_param[np.argmin(tmp_mll)]\n",
    "    mll=np.min(tmp_mll)\n",
    "\n",
    "    return parameters,-mll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af06470",
   "metadata": {},
   "source": [
    "Finally, let's find the ML parameters using this function:|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49132092",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[alpha,beta],mll]=fit_RWmodel(dataset)\n",
    "\n",
    "print(f'Maximum loglikelihood= {mll:.3f}')\n",
    "print('ML parameters')\n",
    "print(f'  -Learning rate= {alpha:.2f}')\n",
    "print(f'  -Inverse temperature parameter= {beta:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a35ed5",
   "metadata": {},
   "source": [
    "### Parameter recovery\n",
    "\n",
    "Before reading too much into the best-fitting parameters, it is important to check whether the fitting procedure gives meaningful parameter values in the best case scenario, -that is, when fitting fake data where the ‘true’ parameter values are known. Such a procedure is known as ‘Parameter Recovery’, and is a crucial part of any model-based analysis.\n",
    "\n",
    "In principle, the recipe for parameter recovery is quite simple. First, simulate fake data with known parameter values. Next, fit the model to these fake data to try to ‘recover’ the parameters. Finally, compare the recovered parameters to their true values. In a perfect world, the simulated and recovered parameters will be tightly correlated, with no bias. If there is only a weak correlation between the simulated and recovered parameters and/or a significant bias, then this is an indication that there is either a bug in your code (which from our own experience is fairly likely) or that the experiment is underpowered to assess this model. Another option is that the model cannot be properly fit (the model parameters are not recoverable), which would mean that we should either consider other models, or create a new experimental design that allow us to recover the parameters.\n",
    "\n",
    "Let's see if the parameters of our RW model are recoverable. First, let's generate a set of random parameters (learning rates and invere temperatures), and let's generate a data with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53570351",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sim=50 #number of simulations\n",
    "ub_beta=10 #upper bound of inverse temperature parameters\n",
    "\n",
    "# Generate the parameters\n",
    "g_parameters=np.array([np.random.rand(n_sim), ub_beta*np.random.rand(n_sim)])\n",
    "\n",
    "# Generate datasets with the parameters\n",
    "dataset=[]\n",
    "for ss in range(n_sim):\n",
    "    # Each element of the list is the dataset for one agent given a set of parameters\n",
    "    dataset.append(RW_simulation (alpha=g_parameters[0,ss],beta=g_parameters[1,ss], n_trials=30, n_blocks=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc0f301",
   "metadata": {},
   "source": [
    "Now let's fit the datasets to get the maximum likelihood parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b699b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize array to save the maximum likelihood parameters\n",
    "mll_parameters=np.full(g_parameters.shape,np.nan)\n",
    "\n",
    "# Find the best fitting paramaters for each dataset\n",
    "for ss in tqdm(range(len(dataset)), desc='Fitting data:'):\n",
    "    [mll_parameters[:,ss],_]=fit_RWmodel(dataset[ss])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00374804",
   "metadata": {},
   "source": [
    "Finally, compare the generating parameters to the fitted parameters to see if the model parameters are recoverable.\n",
    "\n",
    "**Question:** what do you think, are the model parameters recoverable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edc297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_recovery(g_parameters,mll_parameters,labels=[r'Learning rate ($\\alpha$)', r'Inverse temperature ($\\beta$)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb7bcf1",
   "metadata": {},
   "source": [
    "## PART 3: MODEL COMPARISON\n",
    "\n",
    "In model comparison, our goal is to determine which model, out of a set of possible models, is most likely to have generated the data. To illustrate this, we will import a synthetic dataset (i.e., simulated), and we will try to determine which model was more likely to generate it. We will check which of three models explains the dataset better:\n",
    "\n",
    "### Model 1: Random responding\n",
    "\n",
    "In the first model, we assume that participants do not engage with the task at all and simply press buttons at random, perhaps with a bias for one option over the other. Such random behavior is not uncommon in behavioral experiments, especially when participants have no external incentives for performing well. Modeling such behavior can be important if we wish to identify such ‘checked out’ individuals in a quantitative and reproducible manner, either for exclusion or to study the checked-out behavior itself. To model this behavior, we assume that participants choose between the two options randomly, perhaps with some overall bias for one option over the other. This bias is captured with a parameter $b$ (which is between 0 and 1), such that the probability of choosing the two options is:\n",
    "\n",
    "$$p^1_t=b$$\n",
    "\n",
    "$$p^2_t=1−b$$\n",
    "\n",
    "Thus, for two bandits, the random responding model has just one free parameter, controlling the overall bias for option 1 over option 2 ($\\theta_1=b$)\n",
    "\n",
    "\n",
    "### Model 2: Noisy win-stay-lose-shift\n",
    "\n",
    "The win-stay-lose-shift model is one of the simplest models that adapts its behavior according to feedback. Consistent with the name, the model repeats rewarded actions and switches away from unrewarded actions. In the noisy version of the model, the win-stay-lose-shift rule is applied probabilistically, such that the model applies the win-stay-lose-shift rule with probability $1−\\epsilon$, and chooses randomly with probability $\\epsilon$. In the two-bandit case, the probability of staying (i.e., of repeating the same choice from the last trial) is:\n",
    "\n",
    "\n",
    "$$p_t(stay) = \\begin{cases}\n",
    "    1-\\epsilon/2 & \\text{ if } R_{t-1}=1 \\\\\n",
    "    \\epsilon/2 & \\text{ if } R_{t-1}=0\n",
    "\\end{cases}$$\n",
    "\n",
    "Although more complex to implement, this model still only has one free parameter, the overall level of randomness, ($\\theta_2=\\epsilon$).\n",
    "\n",
    "\n",
    "### Model 3: Rescorla-Wagner model\n",
    "\n",
    "Finally, this is the Rescorla-Wagner model that we have already studied, with two parameters, the learning rate $\\alpha$ and the inverse temperature parameter $\\beta$ ($\\theta_3$={$\\alpha$,$\\beta$})."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6ba865",
   "metadata": {},
   "source": [
    "A simplistic approach to model comparison would be to compare the log-likelihoods of each model at the best fitting parameter settings, $p(d_{1:T}|\\theta_m)$. Let's try to do that.\n",
    "\n",
    "First lest load the mystery_model dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7771eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and organize data so that the datasets of the different participants are organized in a list\n",
    "load_data=np.load('mystery_dataset.npy')\n",
    "dataset=[]\n",
    "for ss in range(load_data.shape[3]):\n",
    "    dataset.append(load_data[:,:,:,ss])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab4afe6",
   "metadata": {},
   "source": [
    "Now, let's fit the dataset to our three hypothesis models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac75ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store the maximum loglikelihoods and the parameters\n",
    "mll=np.full(np.array([len(dataset),3]), np.nan)\n",
    "parameters=[]\n",
    "parameters.append(np.full([len(dataset),1], np.nan))\n",
    "parameters.append(np.full([len(dataset),1], np.nan))\n",
    "parameters.append(np.full([len(dataset),2], np.nan))\n",
    "\n",
    "# Fit each participant of the dataset with our 3 models\n",
    "for ss in tqdm(range(len(dataset))):\n",
    "    [parameters[0][ss],mll[ss,0]]=models.fit_REmodel(dataset[ss])\n",
    "    [parameters[1][ss],mll[ss,1]]=models.fit_WSLSmodel(dataset[ss])\n",
    "    [parameters[2][ss,:],mll[ss,2]]=models.fit_RWmodel(dataset[ss])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f34f6ce",
   "metadata": {},
   "source": [
    "Let's now plot the mean maximum loglikelihoods for each model:\n",
    "\n",
    "**Question:** which models seem to explain the data best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54b1667",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.my_barplot(mll,['RE','WSLS','RW'],'Mean MLL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93faee38",
   "metadata": {},
   "source": [
    "However, if the data, used to evaluate the log-likelihood are the same as those used to fit the parameters, then this approach will lead to overfitting, as the model with the most free parameters will almost always fit this ‘training’ data best. As an extreme example, consider the case of a model with one ‘parameter’ per choice, which is the identity of the choice the person actually made. Such a ‘model’ would fit the data perfectly, but would of course tell us nothing about how the choices were actually determined and would make no predictions about what choices would be made in a different setting. Overfitting is a problem in that it decreases the generalizability of the model: it makes it less likely that the conclusions drawn would apply to a different sample.\n",
    "\n",
    "One way to avoid overfitting is to approximately account for the degrees of freedom in the model (e.g., number of parameters, number of measurements...). There are several methods for doing this, but a very common and simple method is the Bayes Information Criterion, BIC, which has an explicit penalty for free parameters.\n",
    "\n",
    "$$BIC=−2*logLL+k_{m}*log(T)$$\n",
    "\n",
    "where LL is the log-likelihood value at the best fitting parameter settings, $T$ is the number of datapoints (i.e., number of trials in the data) and $k_m$ is the number of parameters in model $m$. The model with the smallest BIC score is the model that best fits the data. Thus, the positive effect of km in the last term corresponds to a penalty for models with large numbers of parameters.\n",
    "\n",
    "Let's create a function that calculates the BIC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762800f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BIC (mll,n_params,n):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates the BIC.\n",
    "\n",
    "    Input:\n",
    "    mll: array containing maximum likelihoods where the first dimention corresponds to the different participants, \n",
    "        and the second dimension corresponds to each of the tested models\n",
    "    n_params: array with the number of parameters of each model\n",
    "    n: number of datapoints for each subject\n",
    "\n",
    "    Output:\n",
    "    bic: array with the same dimentions as mll, with the AIC of each model for each subject.\n",
    "    \"\"\"\n",
    "\n",
    "    bic = ## COMPLETE THE CALCULATION OF THE BIC ##\n",
    "    \n",
    "    return bic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262a591d",
   "metadata": {},
   "source": [
    "Let's now calculate the BIC for the different models:\n",
    "\n",
    "**Question:** which model better explains the dataset best now? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c803f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "bic=get_BIC(mll,np.array([1,1,2]),dataset[0][:,:,0].size)\n",
    "plotting.my_barplot(bic,['RE','WSLS','RW'],'BIC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb53e356",
   "metadata": {},
   "source": [
    "### Note: we shouldn't rely on model comparison methods only!\n",
    "\n",
    "Even though model comparison can very useful tool, it's important to remember that it does not tell us **which model generated the data**, it just tells us **which model best explains the data**. We should therefore combine this tool with other analyses that allow us to tell our models apart. A very useful approach is to check whether different models predict different behavioral signatures. If the behavior of different models is not qualitatively different, this is a sign that you should try to design a better experiment. Although not always possible, distinguishing between models on the basis of qualitative patterns in the data is always preferable to quantitative model comparison.\n",
    "\n",
    "For example, we can take the best fitting parameters of each model, and use them to simulate behavior. This will generate a dataset for each model, which will represent the behavior that we would observe if all our \"participants\" followed that model strictly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b5b23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lists to store the simulations of each model\n",
    "RE_sim=[]\n",
    "WSLS_sim=[]\n",
    "RW_sim=[]\n",
    "\n",
    "# Simulate the different models (given their corresponding best fitting parameters) for each participant\n",
    "for ss in range(len(dataset)):\n",
    "    RE_sim.append(models.RE_simulation(b=parameters[0][ss][0]))\n",
    "    WSLS_sim.append(models.WSLS_simulation(eps=parameters[1][ss][0]))\n",
    "    RW_sim.append(models.RW_simulation(alpha=parameters[2][ss,0],beta=parameters[2][ss,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff3809c",
   "metadata": {},
   "source": [
    "And now we can see if the different models predict different behavioral signatures. For instance, let's look again at the probability of repeating a choice as a function of feedback:\n",
    "\n",
    "**Question:** do the dfferent models predict qualitatively different signatures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef51d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted probability of repeat for each model\n",
    "fig,ax = plt.subplots(figsize=(3, 4))\n",
    "plotting.plot_prepeat(RE_sim,ax=ax, color='maroon', label='RE')\n",
    "plotting.plot_prepeat(WSLS_sim,ax=ax, color='gold', label='WSLS')\n",
    "plotting.plot_prepeat(RW_sim,ax=ax, color='teal', label='RW')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6180ff85",
   "metadata": {},
   "source": [
    "Finally, let's see what is the actual behavior in the dataset:\n",
    "\n",
    "**Question:** is any of the models predicting the behavioral signature of the dataset with high fidelity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3978b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model predictions alongside the data\n",
    "fig,ax = plt.subplots(figsize=(3, 4))\n",
    "plotting.plot_prepeat(RE_sim,ax=ax, color='maroon', label='RE')\n",
    "plotting.plot_prepeat(WSLS_sim,ax=ax, color='gold', label='WSLS')\n",
    "plotting.plot_prepeat(RW_sim,ax=ax, color='teal', label='RW')\n",
    "plotting.plot_prepeat(dataset,ax=ax, color='k', label='data')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacee625",
   "metadata": {},
   "source": [
    "## PART 4: EXAMPLE WITH REAL BEHAVIOR\n",
    "\n",
    "Let's now used what we have learned to analyse a real human behavior! We will use the dataset from [Palminteri et al. (PLOS, 2017)](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005684#sec010), where participants completed a two-armed bandit task. We have data from 50 participants, so let's try to model their behavior.\n",
    "\n",
    "First things first, let's load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696dd102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "load_dataset=scipy.io.loadmat(\"P_dataset.mat\")\n",
    "load_dataset=load_dataset['dataset']\n",
    "\n",
    "# Let's structure the dataset in a list, like we have done so far.\n",
    "P_data=[]\n",
    "for ss in range(load_dataset.shape[3]):\n",
    "    P_data.append(load_dataset[:,:,:,ss])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec54bc",
   "metadata": {},
   "source": [
    "Again, let's try to fit our data with the three models from our models comparison section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc62896",
   "metadata": {},
   "outputs": [],
   "source": [
    "mll=np.full(np.array([len(P_data),3]), np.nan)\n",
    "parameters=[]\n",
    "parameters.append(np.full([len(P_data),1], np.nan))\n",
    "parameters.append(np.full([len(P_data),1], np.nan))\n",
    "parameters.append(np.full([len(P_data),2], np.nan))\n",
    "\n",
    "# Fit each participant of the dataset with our 3 models\n",
    "for ss in tqdm(range(len(P_data))):\n",
    "    [parameters[0][ss],mll[ss,0]]=models.fit_REmodel(P_data[ss])\n",
    "    [parameters[1][ss],mll[ss,1]]=models.fit_WSLSmodel(P_data[ss])\n",
    "    [parameters[2][ss,:],mll[ss,2]]=models.fit_RWmodel(P_data[ss])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bb7777",
   "metadata": {},
   "source": [
    "We can now look at the BIC.\n",
    "\n",
    "**Question:** which model provides a best description of the data according to this measure? Does this mean that we can be sure that participants are using this model when completing the task? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5516c62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot the BIC for the different models\n",
    "bic=get_BIC(mll,np.array([1,1,2]),P_data[0][:,:,0].size)\n",
    "plotting.my_barplot(bic,['RE','WSLS','RW'],'BIC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b8fda",
   "metadata": {},
   "source": [
    "We can now take a look at the parameters of the winning model to see how they are distributed across participants. These parameetrs provide a compressed representation of how our participants learn in the task. In our experiment, we could use this parameters in many ways (to check if they change in different conditions, to see if they correlate with certain psychiatric dimensions or neural signatures, etc.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706d0241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the parameters of the winning model\n",
    "fig,ax = plt.subplots(1,2,figsize=(6, 4))\n",
    "plotting.plot_parameters(parameters[2][:,0], labels=[r'$\\alpha$'], colors='teal', ax=ax[0])\n",
    "plotting.plot_parameters(parameters[2][:,1], labels=[r'$\\beta$'], colors='mediumpurple',ax=ax[1])\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7d89c9",
   "metadata": {},
   "source": [
    "Remember that it's always good practice look at some behavioral signatures to see if our models predict participants' behavior! Let's look again at the probability of repeating a choice depending on the feedback received.\n",
    "\n",
    "We first need to create simulations for each model based on the best fitting parameters of the participants. This will give as an estimation of what behavior we would observer if all participants were following a given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6f0c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate behavior based on our hypothesis models\n",
    "RE_sim=[]\n",
    "WSLS_sim=[]\n",
    "RW_sim=[]\n",
    "for ss in range(len(P_data)):\n",
    "    RE_sim.append(models.RE_simulation(b=parameters[0][ss][0]))\n",
    "    WSLS_sim.append(models.WSLS_simulation(eps=parameters[1][ss][0]))\n",
    "    RW_sim.append(models.RW_simulation(alpha=parameters[2][ss,0],beta=parameters[2][ss,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd4f3d",
   "metadata": {},
   "source": [
    "Now, let's look at the probability of repeating a choice:\n",
    "\n",
    "**Question:** what model best predicts participants behavior? Does that model provide a very accurate prediction of participants behavior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a110050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the probability of repeat based on our model simulations and on the actual data\n",
    "fig,ax = plt.subplots(figsize=(3, 4))\n",
    "plotting.plot_prepeat(RE_sim,ax=ax, color='maroon', label='RE')\n",
    "plotting.plot_prepeat(WSLS_sim,ax=ax, color='gold', label='WSLS')\n",
    "plotting.plot_prepeat(RW_sim,ax=ax, color='teal', label='RW')\n",
    "plotting.plot_prepeat(P_data,ax=ax, color='k', label='data')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e3e805",
   "metadata": {},
   "source": [
    "As you can see the prediction of our winning model does not exactly follow the real behavior of participants. There could be several reasons for this, including noise and the fact that different participants may be following different strategies. However, this could also mean that our hypothesis model is missing a key element of the phenomenon we are trying to describe. In order to see if this is the case, we should try other behavioral signatures, take a close look at how the models work and design a new (more complete model).\n",
    "\n",
    "**Question:** could you think of a possible modification that would better account for participants' behavior?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0df57f",
   "metadata": {},
   "source": [
    "One posibility is that participants don't learn equally from rewards an non-rewards. This is, they have a feedback valence bias where they learn more from positive/negative feedback than from its counterpart. A model that could account for this could be something like this:\n",
    "\n",
    "### Model 4: Rescorla Wagner with valence bias\n",
    "\n",
    "This model very similar to our Rescorla Wagner model, but with one key difference: it uses different learning rates when the feedback is rewarding, and when it is non-rewarding. Therefore our learning rule would look something like this:\n",
    "\n",
    "\n",
    "$$\\delta_t = R_t - Q_t(a_t)$$\n",
    "\n",
    "$$Q_{t+1}(a_t) = \\begin{cases}\n",
    "    Q_t(a_t) + \\alpha_+*\\delta_t & \\text{ if } \\delta_t>0 \\\\\n",
    "    Q_t(a_t) + \\alpha_-*\\delta_t & \\text{ if } \\delta_t<0\n",
    "\\end{cases}$$\n",
    "\n",
    "According to this model, positive prediction errors (i.e., $\\delta_t$>0, which happens when we get an outcome greater than expected) can be incorporated into our Q-values at a different rate than negative prediction errors ($\\delta_t$<0, which happen when we get an outcome lower than expected). The free parameter $\\alpha_+$ is the learning rate for positive prediction errors, while the free parameters $\\alpha_-$ is the learning rate for negative prediction errors. \n",
    "\n",
    "Action selection is still computed by feeding the Q-values into a softmax with its corresponding inverse temperature parameter ($\\beta$). This model has therefore three parameters ($\\theta=\\{\\alpha_-,\\alpha_+,\\beta\\}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a59c93",
   "metadata": {},
   "source": [
    "Let's now fit this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aee4674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase our mll and parameters variables so that we can store the new model fitting information\n",
    "mll=mll[:,0:3]\n",
    "mll=np.hstack((mll,np.full(np.array([len(P_data),1]), np.nan)))\n",
    "parameters.append(np.full([len(P_data),3], np.nan))\n",
    "\n",
    "# Fit each participant of the dataset with our new model\n",
    "for ss in tqdm(range(len(P_data))):\n",
    "    [parameters[3][ss,:],mll[ss,3]]=models.fit_RWvalmodel(P_data[ss])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30eae56",
   "metadata": {},
   "source": [
    "We can again look at the BIC to see if this model gives a better description of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a80a1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot the BIC for our previour winning model and our new model\n",
    "bic=get_BIC(mll[:,2:],np.array([2,3]),P_data[0][:,:,0].size)\n",
    "plotting.my_barplot(bic,['RW','RWval'],'BIC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e4810a",
   "metadata": {},
   "source": [
    "Also, we can compute some simulations based on this model (again with the best fitting parameters from participants). We can then look again at our behavioral signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf88977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate behavior based on our new RW valence model\n",
    "RWval_sim=[]\n",
    "for ss in range(len(P_data)):\n",
    "    RWval_sim.append(models.RWval_simulation(alpha_neg=parameters[3][ss,0],alpha_pos=parameters[3][ss,1],beta=parameters[3][ss,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6883c0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the probability of repeat based on our model simulations and on the actual data\n",
    "fig,ax = plt.subplots(figsize=(3, 4))\n",
    "plotting.plot_prepeat(RW_sim,ax=ax, color='teal', label='RW')\n",
    "plotting.plot_prepeat(RWval_sim,ax=ax, color='hotpink', label='RWval')\n",
    "plotting.plot_prepeat(P_data,ax=ax, color='k', label='data')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9455c29a",
   "metadata": {},
   "source": [
    "**Question:** taking all of this into account, can we say that this model seems to give a better description of participants' behavior?\n",
    "\n",
    "Finally, let's look at the parameters from the model.\n",
    "\n",
    "**Question:** do you notice something interesting in the parameters? Do participants' show a bias in learning? in what way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea1e7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model parameters of our RW valence model\n",
    "fig,ax = plt.subplots(1,2,figsize=(6, 4))\n",
    "plotting.plot_parameters(parameters[3][:,0:2], labels=[r'$\\alpha_-$',r'$\\alpha_+$'], colors=['red', 'green'], ax=ax[0])\n",
    "plotting.plot_parameters(parameters[3][:,2], labels=[r'$\\beta$'], ax=ax[1])\n",
    "plt.ylabel('')\n",
    "\n",
    "plotting.plot_parameters(parameters[3][:,1]-parameters[3][:,0], labels=[r'$\\alpha_+$ - $\\alpha_-$'], colors=['orange'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9821fb",
   "metadata": {},
   "source": [
    "## Congratulations! You finished the modelling workshop. \n",
    "\n",
    "This was just a taster of what modelling allows. There are many more methods and models that can be used to study many aspects of human behavior (and cognition). If you want you can take a look at the paper that inspired this workshop ([Ten simple rules for the computational modeling of behavioral data](https://elifesciences.org/articles/49547) or at [Reinforcement learning: an Introduction (Sutton and Barto)](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf), (i.e., the holly Bible of RL research). The next step is to take what you have learned, extend it, and apply it to your own research. Good luck!\n",
    "\n",
    "And remember...\n",
    "\n",
    "![alt text](all_models.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25633efe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
